{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dswh/lil_nlp_with_tensorflow/blob/main/02_02_begin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTguFckTEDWd"
   },
   "source": [
    "# Word Embeddings for Sentiment Analysis\n",
    "\n",
    "This notebook explains how to add the embeddings layer to the neural network. We will train our own word embeddings using a simple Keras model for a sentiment classification task.\n",
    "\n",
    "Steps include:\n",
    "1. Downloading data from tensorflow dataset.\n",
    "2. Segregating training and testing sentences & labels.\n",
    "3. Data preparation to padded sequences\n",
    "4. Defining out Keras model with an Embedding layer.\n",
    "5. Train the model and explore the weights from the embedding layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9mW3Mt2q5kL2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.0.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (5.7.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (3.19.4)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.9.0-py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting etils[epath]\n",
      "  Downloading etils-0.7.1-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 112 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (2.27.1)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorflow_datasets) (1.22.4)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.8.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions; extra == \"epath\" in /usr/local/lib/python3.8/dist-packages (from etils[epath]->tensorflow_datasets) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.9)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21485 sha256=849874303f035f24985ac5ff021eec53bd37ab4b2c445a8bdb36f92a290dc3c1\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: tqdm, dill, googleapis-common-protos, tensorflow-metadata, etils, promise, toml, tensorflow-datasets\n",
      "Successfully installed dill-0.3.5.1 etils-0.7.1 googleapis-common-protos-1.56.4 promise-2.3 tensorflow-datasets-4.6.0 tensorflow-metadata-1.9.0 toml-0.10.2 tqdm-4.64.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "##import the required libraries and APIs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "!pip install tensorflow_datasets\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rhw0j_s5UZ2"
   },
   "source": [
    "## Downloading the TensorFlow `imdb_review` dataset\n",
    "\n",
    "> Make sure tensorflow_datasets is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dx_DJfb7EFHh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dea47ca602541c79f084b5fab2de707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6030b4f0c5134e639ca444b5d3253cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteLG55E8/imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteLG55E8/imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteLG55E8/imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##load the imdb reviews dataset\n",
    "data, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MBqFTBP6DT4"
   },
   "source": [
    "## Segregating training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GM2X1wLvUb8n"
   },
   "outputs": [],
   "source": [
    "##segregate training and test set\n",
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "##create empty list to store sentences and labels\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rxoAZl0gU_y-"
   },
   "outputs": [],
   "source": [
    "##iterate over the train data to extract sentences and labels\n",
    "for sent, label in train_data:\n",
    "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "##iterate over the test set to extract sentences and labels\n",
    "for sent, label in test_data:\n",
    "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    test_labels.append(label.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eDKl0NzBITfe"
   },
   "outputs": [],
   "source": [
    "##convert lists into numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLIjftvF6IRZ"
   },
   "source": [
    "## Data preparation - setting up the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Mqx-tgBVXQz"
   },
   "outputs": [],
   "source": [
    "##define the parameters for the tokenizing and padding\n",
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 150\n",
    "trunc_type='post'\n",
    "oov_tok = \"<oov>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nYsZatAaVmfq"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "##training sequences and labels\n",
    "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_seqs, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "##testing sequences and labels\n",
    "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seqs,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U7DbJ3cWV4zC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0  11  26  75 571   6 805   1 313 106  19  12   7 629 686   6   4\n",
      "   1   5 181 584  64   1 110   1   3   1  21   2   1   3 258  41   1   4\n",
      " 174 188  21  12   1  11   1   1  86   2  20  14   1   2 112 940  14   1\n",
      "   1 548   3 355 181 466   6 591  19  17  55   1   5  49  14   1  96  40\n",
      " 136  11 972  11 201  26   1 171   5   2  20  19  11 294   2   1   5  10\n",
      "   3 283  41 466   6 591   5  92 203   1 207  99 145   1  16 230 332  11\n",
      "   1 384  12  20  31  30]\n",
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? i have been known to fall <oov> during films but this is usually due to a <oov> of things including really <oov> being <oov> and <oov> on the <oov> and having just <oov> a lot however on this <oov> i <oov> <oov> because the film was <oov> the plot development was <oov> <oov> slow and boring things seemed to happen but with no <oov> of what was <oov> them or why i admit i may have <oov> part of the film but i watched the <oov> of it and everything just seemed to happen of its own <oov> without any real <oov> for anything else i <oov> recommend this film at all\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_padded[1])\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcvfYesOIo3A"
   },
   "source": [
    "## Define the Neural Network with Embedding layer\n",
    "\n",
    "1. Use the Sequential API.\n",
    "2. Add an embedding input layer of input size equal to vocabulary size.\n",
    "3. Add a flatten layer, and two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RF6ict6vWJAV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 16)           16000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 14406     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,413\n",
      "Trainable params: 30,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'), # for classification\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "##compile the model with loss function, optimizer and metrics\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlNRXgJ99Dv9"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2S9zFDyLWZDF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.5193 - accuracy: 0.7256 - val_loss: 0.3636 - val_accuracy: 0.8389\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3689 - accuracy: 0.8370 - val_loss: 0.3694 - val_accuracy: 0.8355\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.3170 - accuracy: 0.8680 - val_loss: 0.3785 - val_accuracy: 0.8290\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2558 - accuracy: 0.8994 - val_loss: 0.4251 - val_accuracy: 0.8166\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 7s 10ms/step - loss: 0.1968 - accuracy: 0.9282 - val_loss: 0.4820 - val_accuracy: 0.8088\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1433 - accuracy: 0.9534 - val_loss: 0.5551 - val_accuracy: 0.8015\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.1012 - accuracy: 0.9699 - val_loss: 0.6548 - val_accuracy: 0.7924\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0668 - accuracy: 0.9835 - val_loss: 0.7698 - val_accuracy: 0.7912\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 7s 10ms/step - loss: 0.0447 - accuracy: 0.9904 - val_loss: 0.9513 - val_accuracy: 0.7862\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.0302 - accuracy: 0.9938 - val_loss: 1.0177 - val_accuracy: 0.7859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18f57f9ee0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "##train the model with training and validation set\n",
    "model.fit(\n",
    "    train_padded, \n",
    "    train_labels, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(test_padded, test_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCHKeE8Z9Gm9"
   },
   "source": [
    "## Deriving weights from the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "X6aT5Q63gW8j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n",
      "[[ 0.01653522 -0.00327209 -0.05449872 ... -0.01003235 -0.00121851\n",
      "  -0.01222734]\n",
      " [ 0.03183649  0.01428665 -0.02701611 ... -0.01070418  0.02977063\n",
      "   0.01566753]\n",
      " [ 0.01609767 -0.0218314  -0.05049689 ... -0.03955908  0.04542179\n",
      "  -0.03035884]\n",
      " ...\n",
      " [-0.1315037   0.21381466  0.12779288 ... -0.10847312 -0.17082211\n",
      "  -0.22325724]\n",
      " [-0.01705533  0.02146531 -0.15523589 ... -0.21621649  0.04260562\n",
      "  -0.19332454]\n",
      " [-0.02548479  0.08473054  0.2005548  ...  0.0978324   0.01761167\n",
      "   0.31239313]]\n"
     ]
    }
   ],
   "source": [
    "##isolating the first embedding layer\n",
    "l1 = model.layers[0]\n",
    "\n",
    "##extracting learned weights\n",
    "weights = l1.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGXEpSUgZMH0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPR6DrF/TCdxoVC8kbwGLAj",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_02_begin.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
