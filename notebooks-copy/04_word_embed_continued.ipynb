{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dswh/lil_nlp_with_tensorflow/blob/main/02_03_begin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTguFckTEDWd"
   },
   "source": [
    "# Projecting embeddings on TensorFlow projector\n",
    "\n",
    "This notebook explains how you can write the vectors of an embeddings into a TSV file to visualise it in a 3D space on the [TensorFlow projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mW3Mt2q5kL2",
    "outputId": "735f42fd-4947-4fc9-cf68-55c4519b932c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.4.3)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.1)\n",
      "Requirement already satisfied: google-colab in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
      "Collecting tensorflow.datasets\n",
      "  Downloading tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 931 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.46.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (62.3.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: google-auth~=1.4.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (1.4.2)\n",
      "Requirement already satisfied: requests~=2.21.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (2.21.0)\n",
      "Requirement already satisfied: ipykernel~=4.6.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (4.6.1)\n",
      "Requirement already satisfied: ipython~=5.5.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (5.5.0)\n",
      "Requirement already satisfied: portpicker~=1.2.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (1.2.0)\n",
      "Requirement already satisfied: tornado~=4.5.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (4.5.3)\n",
      "Requirement already satisfied: notebook~=5.2.0 in /usr/local/lib/python3.8/dist-packages (from google-colab) (5.2.2)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[K     |████████████████████████████████| 95 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from tensorflow.datasets) (5.7.1)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.7.1-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.9.0-py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth~=1.4.0->google-colab) (5.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth~=1.4.0->google-colab) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth~=1.4.0->google-colab) (4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests~=2.21.0->google-colab) (2022.5.18.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests~=2.21.0->google-colab) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests~=2.21.0->google-colab) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests~=2.21.0->google-colab) (2.8)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel~=4.6.0->google-colab) (5.2.1.post0)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel~=4.6.0->google-colab) (7.3.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (5.1.1)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (1.0.18)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.8/dist-packages (from ipython~=5.5.0->google-colab) (0.8.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (3.1.2)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (4.4.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (6.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (0.2.0)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (0.15.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from notebook~=5.2.0->google-colab) (4.10.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow.datasets) (3.8.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=1.4.0->google-colab) (0.4.8)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (0.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (1.5.5)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel~=4.6.0->google-colab) (23.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect; sys_platform != \"win32\"->ipython~=5.5.0->google-colab) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython~=5.5.0->google-colab) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->notebook~=5.2.0->google-colab) (2.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook~=5.2.0->google-colab) (4.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (1.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (4.11.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (5.0.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.6.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.2.2)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook~=5.2.0->google-colab) (1.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook~=5.2.0->google-colab) (21.4.0)\n",
      "Requirement already satisfied: webencodings>=0.4 in /usr/local/lib/python3.8/dist-packages (from tinycss2->nbconvert->notebook~=5.2.0->google-colab) (0.5.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->nbconvert->notebook~=5.2.0->google-colab) (2.3.2.post1)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21485 sha256=1e74cfb8ad7e2647be6ddb2844b100b471116d514ef2816e5d5240503ac76ff9\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: dill, tqdm, promise, toml, etils, googleapis-common-protos, tensorflow-metadata, tensorflow.datasets\n",
      "Successfully installed dill-0.3.5.1 etils-0.7.1 googleapis-common-protos-1.56.4 promise-2.3 tensorflow-metadata-1.9.0 tensorflow.datasets toml-0.10.2 tqdm-4.64.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "##import the required libraries and APIs\n",
    "!pip install pandas tensorflow google-colab tensorflow_datasets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rhw0j_s5UZ2"
   },
   "source": [
    "## Downloading the TensorFlow `imdb_review` dataset\n",
    "\n",
    "> Make sure tensorflow_datasets is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dx_DJfb7EFHh"
   },
   "outputs": [],
   "source": [
    "##load the imdb reviews dataset\n",
    "data, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MBqFTBP6DT4"
   },
   "source": [
    "## Segregating training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GM2X1wLvUb8n"
   },
   "outputs": [],
   "source": [
    "##segregate training and test set\n",
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "##create empty list to store sentences and labels\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rxoAZl0gU_y-"
   },
   "outputs": [],
   "source": [
    "##iterate over the train data to extract sentences and labels\n",
    "for sent, label in train_data:\n",
    "    train_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "##iterate over the test set to extract sentences and labels\n",
    "for sent, label in test_data:\n",
    "    test_sentences.append(str(sent.numpy().decode('utf8')))\n",
    "    test_labels.append(label.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eDKl0NzBITfe"
   },
   "outputs": [],
   "source": [
    "##convert lists into numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLIjftvF6IRZ"
   },
   "source": [
    "## Data preparation - setting up the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6Mqx-tgBVXQz"
   },
   "outputs": [],
   "source": [
    "##define the parameters for the tokenizing and padding\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nYsZatAaVmfq"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "##training sequences and labels\n",
    "train_seqs = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_seqs,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "##testing sequences and labels\n",
    "test_seqs = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seqs,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7DbJ3cWV4zC",
    "outputId": "291669ca-9b81-4abe-9901-eae26987f8e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "[   0    0    0    0    0    0    0    0   11   26   75  571    6  805\n",
      " 2354  313  106   19   12    7  629  686    6    4 2219    5  181  584\n",
      "   64 1454  110 2263    3 3951   21    2    1    3  258   41 4677    4\n",
      "  174  188   21   12 4078   11 1578 2354   86    2   20   14 1907    2\n",
      "  112  940   14 1811 1340  548    3  355  181  466    6  591   19   17\n",
      "   55 1817    5   49   14 4044   96   40  136   11  972   11  201   26\n",
      " 1046  171    5    2   20   19   11  294    2 2155    5   10    3  283\n",
      "   41  466    6  591    5   92  203    1  207   99  145 4382   16  230\n",
      "  332   11 2486  384   12   20   31   30]\n",
      "? ? ? ? ? ? ? ? i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_padded[1])\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcvfYesOIo3A"
   },
   "source": [
    "## Define the Neural Network with Embedding layer\n",
    "\n",
    "1. Use the Sequential API.\n",
    "2. Add an embedding input layer of input size equal to vocabulary size.\n",
    "3. Add a flatten layer, and two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF6ict6vWJAV",
    "outputId": "9bd86d50-8f79-451d-c09e-c50c095a495e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "##compile the model with loss function, optimizer and metrics\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlNRXgJ99Dv9"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2S9zFDyLWZDF",
    "outputId": "1fd0ac75-0d3f-451f-8d4c-73cdca39c719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 5s 5ms/step - loss: 0.4856 - accuracy: 0.7498 - val_loss: 0.3463 - val_accuracy: 0.8505\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.2420 - accuracy: 0.9070 - val_loss: 0.3652 - val_accuracy: 0.8407\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.1028 - accuracy: 0.9717 - val_loss: 0.4640 - val_accuracy: 0.8234\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0269 - accuracy: 0.9962 - val_loss: 0.5169 - val_accuracy: 0.8281\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0071 - accuracy: 0.9995 - val_loss: 0.5828 - val_accuracy: 0.8271\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 0.8295\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.6778 - val_accuracy: 0.8302\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 5.1891e-04 - accuracy: 1.0000 - val_loss: 0.7187 - val_accuracy: 0.8300\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 2.9927e-04 - accuracy: 1.0000 - val_loss: 0.7565 - val_accuracy: 0.8306\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 1.8063e-04 - accuracy: 1.0000 - val_loss: 0.7938 - val_accuracy: 0.8305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faff8288eb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "##train the model with training and validation set\n",
    "model.fit(\n",
    "    train_padded, \n",
    "    train_labels, \n",
    "    epochs=num_epochs, \n",
    "    validation_data=(test_padded, test_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCHKeE8Z9Gm9"
   },
   "source": [
    "## Deriving weights from the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6aT5Q63gW8j",
    "outputId": "301f95ca-73a2-4e70-d05c-11ccd316eb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n",
      "[-0.04851312  0.00704478  0.01894414 -0.0144157   0.03672409  0.01898086\n",
      "  0.03145445 -0.00181298  0.04810522 -0.00727019 -0.02791358  0.00011839\n",
      "  0.01699063  0.09089278 -0.00744939 -0.01572671]\n"
     ]
    }
   ],
   "source": [
    "##isolating the first embedding layer\n",
    "l1 = model.layers[0]\n",
    "\n",
    "##extracting learned weights\n",
    "weights = l1.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
    "print(weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDbASWaV-hqW"
   },
   "source": [
    "## Downloading the vectors and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_U2cCtIY9bA3"
   },
   "outputs": [],
   "source": [
    "##import I/O module in python\n",
    "import io\n",
    "\n",
    "##open the text stream for vectors\n",
    "vectors = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "##open the text stream for metadata\n",
    "meta = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "##write each word and its corresponding embedding\n",
    "for index in range(1, vocab_size):\n",
    "  word = reverse_word_index[index]  # flipping the key-value in word_index\n",
    "  embeddings = weights[index]\n",
    "  meta.write(word + \"\\n\")\n",
    "  vectors.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "\n",
    "##close the stream\n",
    "vectors.close()\n",
    "meta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "VMz_eEKS-koj",
    "outputId": "328fd137-444e-4a6c-b3dc-d9be83fa5baf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m   \u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvectors.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m   files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/files.py:159\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    156\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    157\u001b[0m started\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m--> 159\u001b[0m \u001b[43m_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_js\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;43;03m    (async function() {\u001b[39;49;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;43;03m      const response = await fetch('https://localhost:%(port)d%(path)s');\u001b[39;49;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;43;03m      if (!response.ok) {\u001b[39;49;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;43;03m        throw new Error('Failed to download: ' + response.statusText);\u001b[39;49;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;43;03m      }\u001b[39;49;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;43;03m      const blob = await response.blob();\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;43;03m      const a = document.createElement('a');\u001b[39;49;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;43;03m      a.href = window.URL.createObjectURL(blob);\u001b[39;49;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;43;03m      a.download = '%(name)s';\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;43;03m      document.body.appendChild(a);\u001b[39;49;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;43;03m      a.click();\u001b[39;49;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;43;03m      a.remove();\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;43;03m    })();\u001b[39;49;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/output/_js.py:39\u001b[0m, in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_result:\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_reply_from_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py:101\u001b[0m, in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m reply \u001b[38;5;241m=\u001b[39m _read_next_input_message()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;241m==\u001b[39m _NOT_READY \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(reply, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m   \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.025\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (reply\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolab_reply\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     reply\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolab_msg_id\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m message_id):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##download the written files to your local machine\n",
    "# !pip install google.colab\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('meta.tsv')\n",
    "    \n",
    "#After downloading these files, load them to https://projector.tensorflow.org to see \n",
    "#a visualization of each sample in the 2D or 3D space after PCA or other method"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAF3V3sqvsZcS2m7RI+CGm",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "02_03_begin.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
